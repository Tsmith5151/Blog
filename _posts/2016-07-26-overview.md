---
layout: post
section-type: post
title: Machine Learning Algorithms & Concepts
category: technology
tags: [ 'Machine Learning' ]
author: Trace Smith
---

In the following section, I define commonly used terms in Machine Learning. Although this only covers a small poriton of the terms, more will be added to this library over time. Also included in this blog is Machine Learning algorithms that are often utilized for supervised and unsupervised learning problems. Again, this section will be expanded upon further with time. 

### ***Terms***

<span style="color: #00b3b3">**Cross-Validation:**</span> To avoid sampling issues, which can cause the training-set to be too optimistic. Cross-validation is used to protect against overfitting in a predictive model, particularly the case where the amount of data is limited. So in cross-validation, you identify the fix number of folds of the data, run the algorithm on each fold, and then average the overall error estimate.

<span style="color: #00b3b3">**Gridsearch:**</span> It is desirable to find the optimal combination of the hyperparameters during training. Gridsearch is an exhaustive search that trains and evaluates a model for all possible combination of hyperparameters that produce the best model. It is a systematic search method that combines all possible combinations of hyperparameters into individual sets (i.e. max_depth, number leaves to split etc.) 

<span style="color: #00b3b3">**Machine Learning:**</span> Machine learning is the science of getting computers to act without being explicitly programmed. Consists of (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI).

<span style="color: #00b3b3">**Overfitting:**</span> At the instance where the model begins to behave too much like the training data then it might be observed that overfitting is occurring as the model does not perform well when testing on out of sample data, thus the error rates will begin to significantly differ. A decrease in the training error implies that the model is becoming better at fitting the data; when the testing error levels-out and is no longer decreasing, additional knowledge is not gained on the out-of-sample data. If the error does not reduce any further during testing, then the complexity is increased for no reason and therefore overfitting occurs.

<span style="color: #00b3b3">**Reinforcement Learning:**</span> is learning by interacting with the environment. An agent learns from the consequences of its actions, rather than from being explicitly taught and it selects its actions on basis of its past experiences (exploitation) and also by new choices (exploration), which is essentially trial and error learning.

<span style="color: #00b3b3">**Supervised Learning:**</span> We have a dataset that consists of a list of features and we are trying to predict the label and the labels supervises the learning. So, given a bunch of x,y pairs and the goal is to find some function (f) that will map some new x to a proper y – “Function Approximation”. ***Example:*** the teacher shows student 10 objects and told these 5 are cars and these 5 are houses. The student looks at the objects and figures what features of the house makes its characteristics a car and the same for a house.

<span style="color: #00b3b3">**Unsupervised Learning:**</span> There are no labels (y), not interested in predictions, interested in how the features are related.  Given a bunch of X’s and the goal is to find some function (f), for “Clustering”. ***Example:*** the teacher shows student 10 objects but isn’t told these 5 are cars and these 5 are houses: The student looks at the objects and tries to figure out some pattern between the two

### ***Supervised Learning Algorithms***

<span style="color: #00b3b3">**Decision Trees: Classification:**</span> A simplistic explanation is that we can think of this model as breaking down our data by making decisions based on asking a series of questions. First, the decision tree algorithm is a top-down approach (ID3); meaning we start at the tree root and split the data on the feature that results in the largest information gain. It is an iterative process and we can then repeat the splitting criteria at each child node until the leaves are pure; meaning that the samples at each node all belong to the same class. As the decision tree grows and becomes more complex the issue of overfitting arises. In this scenario, the model has virtually memorized the training data but will not be expected to perform well with out-of-sample data. On the other hand, if the tree is too simple then this could result in underfitting as the learning value is restricted to one level of the decision tree and does not allow the training set to learn the data adequately; a lower complexity decision tree results in high bias. Therefore we want to prune the tree by setting a limit for the maximum depth of the tree. One way is that we can observe the error vs max_depth plots and also implement Gridsearch to identify the optimal depth. 

Starting at the top node, we partition the data into subsets. For instance, we want to predict if we would go outside and play tennis today based on a series of attributes. Say the root feature we are splitting on is “outcome” and there are three different class labels: sunny, overcast, and rainy; we partition the data into these three subsets. The next question is the subset pure? Meaning do you play tennis every time it is sunny (i.e. either all positive or all negative)? If so, we stop and try to find another attribute to split further out of those training samples. The splitting criteria is based on the information gain, which is a mathematical way of capturing the amount of information one gains by picking a particular attribute. For classification type features, one metric we can use is entropy, a common impurity measure for splitting criteria (Sum(-pilog2pi)). The lower the entropy, the more predictable the class is and for higher entropy values, it becomes more unpredictable. Computing the difference between the entropies before and after the split yields the information gain. Our objective function is to maximize the information gain at each split, thus the attribute with the highest change in entropy is used as the splitting criteria. Note: We can only split on one attribute at a time (i.e. horizontal or vertical boundary lines). It will take a while to get the separation that a regression or Naïve Bayes algorithm that would take pretty quickly. Therefore, you will need a tree with lots of nodes, and however, as increasing the complexity of the tree increases, so does the likelihood of overfitting of the data.   

Several performance metrics on how well the model generalizes to out-of-sample data: *Precision:* How many returned documents are true (i.e. of those selected for intervention, how many actually need intervention). *Recall:* How many positives does the model return (i.e. of those needing intervention, how many of them were identified). *F1:* harmonic mean of precision to recall (use this performance metric when labels are imbalanced).

We can also look at the *Confusion Matrix* which can be in the form of a heatmap and indicates the numbers of true positives, true negatives, false positives, and false negatives for each class label; this gives a more detailed analysis than calculating the accuracy, which is just computing the proportion of correct guesses. Accuracy can give some misleading results, especially when the dataset is unbalances and is not a reliable metric when examining the performance of a classifier.

<span style="color: #00b3b3">**Decision Trees: Regression:**</span> This algorithm is takes the same approach as the Classification trees, however now we replace the information fain with the standard deviation reduction. Remember, in regression we want to predict a continuous dependent variable from a number of independent variables. For example, instead now of predicting whether or not we will play tennis today, we predict how many hours will you play. Now lets discuss the splitting criteria. First, we compute the standard deviation of the target column.  Next, the root node (i.e. outlook) is then split into subsets. The standard deviation for each subset variable is calculated. Finally, the computed standard deviation is then subtracted from the standard deviation before the split. The attribute with the highest standard deviation reduction is then used to split at the decision node. This process is repeated until each leaf is pure or a specified max_depth is given.


*Example of Decision Tree*

<img src = "https://tsmith5151.github.io/Blog/img/ML_Concepts/Tree.png">

<span style="color: #00b3b3">**K-Nearest Neighbor:**</span> Is an example of a lazy learner; not because of its apparent simplicity, because it memorizes the training dataset instead of learning a discriminative function from the training data. The algorithm beings by choosing a distance metric, then the KNN algorithm finds the specified k samples in the training dataset that are closest to the point that we want to classify. The class label of the new data point is then determined by a majority vote among its k nearest neighbors. A summary of the steps are as follows:

	1. Choose the number of k and a distance metric.
	2. Find the k nearest neighbors of the sample that we want to classify.
	3. Assign the class label by majority vote.


<span style="color: #00b3b3">**Naïve Bayes:**</span> Is a classification algorithm, which is based on Bayes’ rule of conditional probability. It makes the assumption that each feature is conditionally independent from each other. As it is often stated, Naive Bayes is not so naive as the algorithm assumes independence between the features, which may be to simplistic and not always hold true, but it works surprisingly well in practice. Naïve Bayes algorithm works by taking all of the evidence available (attributes) in order to modify the prior probability of the prediction. The prior probability is defined as the probability of the hypothesis without knowing any of the evidence. Therefore, using Bayes Rule and assuming that the features are independent (given the class), their combined probability is obtained by multiplying the prior probability and the probabilities of the new evidence (testing example) given the hypothesis.

<img src = "https://tsmith5151.github.io/Blog/img/ML_Concepts/NB.png">

*Strength:* One advantage of using NB is that the computational complexity is lower than other methods such as decision trees and therefore is quite fast to run and does not require a lot of CPU memory. 

*Weakness:* One drawback when using NB is the model may be too simplistic and under-fit the data given smaller datasets. Also, if a particular attribute value does not occur in the training set with every class value, things go badly. 

<span style="color: #00b3b3">**Random Forest:**</span> An adaptation of decision trees. Intuitively, a random forest can be considered as a collection or an ensemble of decision trees. This approach greatly reduces the variances and bias of the estimates, especially for unbalanced data. Typically, as the larger the number of trees increase, then the better the performance of the classifier will be, however this can result in an increased computational cost. Here is a summary of the RF algorithm:

	1.) Take the set of training examples and bootstrap random samples into k subsets
	2.) Grow a full decision tree from the bootstrap sample (with no pruning)
	3.) At each node: Randomly select d features without replacement
	4.) Split the node using the feature that provides the best split according to the objective function, for instance, by maximizing the information gain
	5.) At prediction time, classify new data point using all of the k trees and then use the majority vote 

*Strength:* One big advantage of random forests is that we don't have to worry so much about choosing good values of the hyperparameters. Also, we typically don't need to prune the random forest since the ensemble model is quite robust to noise from the individual decision trees. 

*Weakness:* Doesn’t offer the same level of interpretability as decision trees

<span style="color: #00b3b3">**Support Vector Machine:**</span> Is a classification algorithm that constructs hyperplanes in a high dimensional space and separates two different class labels. There are many possible solutions on where to draw the decision boundary, such as the solid line on the left or to the right. SVM's reduces the risk of selecting the wrong decision boundary by choosing the line that has the largest distance from the bordering data points of the two classes. Our optimization objective is to maximize the margin. The margin is defined as the distance between the separating hyperplane and the training samples that are closest to this hyperplane, which are the so-called support vectors. The dashed line in the middle of the margin width would be where the decision boundary would be. So the idea to have decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting. Thus having the additional space between the groups reduces the chance of selecting the wrong class. 

Finally, note that we would not be able to separate samples from the positive and negative class very well using a linear hyperplane as the decision boundary via linear SVM models. Therefore, implementing the kernel trick, we can solve non-linear classification problems. The basic idea behind kernel methods to deal with such linearly inseparable data is to create nonlinear combinations of the original features to project them onto a higher dimensional space via a mapping function where it becomes linearly separable.

##### Source: Python Machine Learning (Raschka)
<img src = "https://tsmith5151.github.io/Blog/img/ML_Concepts/SVM.png">

### ***Unsupervised Learning Algoirthms***

<span style="color: #00b3b3">**K-Means:**</span> An iterative clustering algorithm that groups samples which consist of similar characteristics and that are more related to each other than in other groups. Each group in the data is distributed around a central point called the "centroid" which is the average of the cluster. The steps are as follows:

	1. Specify the number of clusters ‘k’
	2. Randomly pick k centroids from the data points as initial cluster centers
	3. Assign each sample to the nearest centroid (i.e. Euclidian distance)
	4. Move the centroids to the center of the samples that were assigned to it
	5. Repeat the third and fourth steps until the cluster assignment 

We can find the optimal number of clusters by plotting performance metrics such as the Sum Squared Error vs the number of specified clusters. This should result in an elbow shaped plot; thus at inflection point of the plot will result in the optimal number of clusters. 

*Strength:* Due to it's simplicity, speed, and scalability of larger numbers of data points. K-mean is a hard clustering algorithm, where the clusters to not overlap (i.e. the data point is either "blue" or "green". 

*Weakness:* The initial centroids are placed randomly, which could result in a bad starting spot and the iteration could stop at an unlikely solution. Thus numerous iterations need to be run which can come at a cost of more time required to run K-means, however the trade-off is that more iteration that are conducted, the better the results will be.

<span style="color: #00b3b3">**Gaussian Mixture Model:**</span> Assumes that all of the data points in each cluster are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. GMM use a Gaussian distribution to find the most probable cluster that a point would belong to. The parameters of the GMM are estimated by the maximum likelihood criterion using the Expectation-Maximization (EM) algorithm. 

*Strengths:* EM algorithm is very sensitive to the initialization of the model, thus it can require a longer time to converge if there is a poor initial guess. Also, using mixture models (Gaussian distribution) it is a probabilistically way of doing soft clustering. Soft clustering is often desired as it provides uncertainties on the assignment of data points to a given cluster rather than K-means, which classifies the point of interest to belong to only a single cluster.

### ***Reinforcement Learning***

<span style="color: #00b3b3">**Q-Learning:**</span> Is an algorithm where the agent attempts to learn what the optimal policy is from its history of interacting with the environment. We first initialize the Q-Table, which is the backbone of the Q-Learning Algorithm. This is what stores all the Q-Values for any given state/action pair the agent will encounter in the environment. In order to start populating these values with meaningful numbers the agent needs to randomly select an action at any given state and collect the associated reward. If the action was bad then the Q-Value that state/action pair will decrease. On the other hand, if the action was good then the opposite happens and the Q-Value increases. Eventually at some point the agent needs to stop exploring and start exploiting the values and information in the Q-Table. This is where the policies such as epsilon-greed come into play. The following are the steps to implement Bellman's Q-learning algorithm:

	1. Initialize the Q-values; Q(s, a)
	2. Observe the current state (s)
	3. For the current state, choose an action based on the selection policy (ε-greedy)
	4. Take the action, and observe the reward and as the new state (s')
	5. Update the Q-value for the state using the observed reward and the maximum reward possible for the next state. 
	6. Set state to = new state, and repeat steps until smartcab arrives at destination 


### ***Feature Extraction***

<span style="color: #00b3b3">**Principal Component Analysis:**</span> PCA is a widely used technique designed to reveal the underlying structure presumed to exist within a set of multivariate measurements. The objective of PCA is to create a new set of uncorrelated variables that retain as much of the information as possible in the original set of measurements. PCA attempts to find the directions of maximum variance in a high-dimensional data (d) and then projects it onto a new subspace (k<=d). Principal components are nothing more than the extracted eigenvectors and eigenvalues from a correlation matrix. The eigenvectors of the covariance matrix are each associated with an eigenvalue, which tell us about the length or the magnitude of the eigenvectors. We are interested in keeping only those eigenvectors with the much larger eigenvalues as they contain more information about the data. 

To summarize, the objective of PCA is to reduce the dimensionality of the data by compressing it onto a new feature subspace, we chose only the subset of the principal components (eigenvectors) that accounts for the highest variance. The eigenvector corresponding to the highest eigenvalue will be the first PC and will point in the direction of maximum variance and the second eigenvector will point orthogonal to the first principal component. After applying the linear PCA transformation, we have a lower dimensional subspace where the samples are “most spread” along the new feature axes. Reducing high dimensional space down to two or three principal components without losing much information we can then visualize the data graphically. 


<span style="color: #00b3b3">**Independent Component Analysis:**</span> Is a statistical technique for identifying the underlying hidden factors for a given multidimensional dataset. In short, ICA produces dimensions of variation where the dimensions (features) are statistically independent from one another. The assumption is that the latent variables are both non-Gaussian and mutually independent; these variables are called the independent components.  One way to help you interpret the ICA results is to look at them as dimensions of variation. So if a feature has a large positive value and another has a large negative value it would suggest independent of other effects that they a strong inverse relationship between each other. For example, there are buyers who mostly purchased Milk and Grocery, and less Detergents/Paper or Delicatessen. Likely this could represent the type of purchasers for a small market or convenience store.

