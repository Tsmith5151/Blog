---
layout: post
section-type: post
title: Overview of Machine Learning Algorithms and Concepts
category: technology
tags: [ 'Machine Learning' ]
author: Trace Smith
---

**Machine Learning:** Machine learning is the science of getting computers to act without being explicitly programmed. Consists of (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI).

**Supervised Learning:** We have a dataset that consists of a list of features and we are trying to predict the label and the labels supervises the learning ---> So, given a bunch of x,y pairs and the goal is to find some function (f) that will map some new x to a proper y – “Function Approximation”  

	Example: Teacher show student 10 objects: 5 are cars and 5 are houses -> The student looks at the objects and figures what features of the house makes its characteristics a car and the same for a house.  

**Unsupervised Learning:** There are no labels (y), not interested in predictions, interested in how the features are related.  Given a bunch of X’s and the goal is to find some function (f), for “Clustering”

	Example: Student is shown 10 objects and isn’t told these 5 are cars and these 5 are houses: -> The student looks at the objects and tries to figure out some pattern
	
**Reinforcement Learning:** is learning by interacting with the environment. An agent learns from the consequences of its actions, rather than from being explicitly taught and it selects its actions on basis of its past experiences (exploitation) and also by new choices (exploration), which is essentially trial and error learning.

**Cross-Validation:** To avoid sampling issues, which can cause the training-set to be too optimistic. Cross-validation is used to protect against overfitting in a predictive model, particularly the case where the amount of data is limited. So in cross-validation, you identify the fix number of folds of the data, run the algorithm on each fold, and then average the overall error estimate.

**Gridsearch:** It is desirable to find the optimal combination of the hyperparameters during training. Gridsearch is an exhaustive search that trains and evaluates a model for all possible combination of hyperparameters that produce the best model. It is a systematic search method that combines all possible combinations of hyperparameters into individual sets (i.e. max_depth, number leaves to split etc.) 

**Overfitting:** At the instance where the model begins to behave too much like the training data then it might be observed that overfitting is occurring as the model does not perform well when testing on out of sample data, thus the error rates will begin to significantly differ. A decrease in the training error implies that the model is becoming better at fitting the data; when the testing error levels-out and is no longer decreasing, additional knowledge is not gained on the out-of-sample data. If the error does not reduce any further during testing, then the complexity is increased for no reason and therefore overfitting occurs.

<img src = "https://tsmith5151.github.io/Blog/img/ML_Concepts/Depth1.png">
<img src = "https://tsmith5151.github.io/Blog/img/ML_Concepts/Depth2.png">


### Algorithms:

**DecisionTree:** A simplistic explanation is that we can think of this model as breaking down our data by making decisions based on asking a series of questions. First, the decision tree algorithm is a top-down approach; meaning we start at the tree root and split the data on the feature that results in the largest information gain. It is an iterative process and we can then repeat the splitting criteria at each child node until the leaves are pure; meaning that the samples at each node all belong to the same class. As the decision tree grows and becomes more complex the issue of overfitting arises. In this scenario, the model has virtually memorized the training data but will not be expected to perform well with out-of-sample data. On the other hand, if the tree is too simple then this could result in underfitting as the learning value is restricted to one level of the decision tree and does not allow the training set to learn the data adequately; a lower complexity decision tree results in high bias. Therefore we want to prune the tree by setting a limit for the maximum depth of the tree. One way is that we can observe the error vs max_depth plots and also implement Gridsearch to identify the optimal depth. 

***For the splitting criteria:*** The splitting criteria is based on the information gain, which is a mathematical way of capturing the amount of information one gains by picking a particular attribute. Entropy is one of the common impurity measures for splitting criteria (-pilog2pi). The lower the entropy, the more predictable the class is and for higher entropy values, it becomes more unpredictable. Computing the difference between the entropies before and after the split yields the information gain. Our objective function is to maximize the information gain at each split, thus the attribute with the highest change in entropy is used as the splitting criteria. 

<img src = "https://tsmith5151.github.io/Blog/img/ML_Concepts/tree.png">

**K-Nearest Neighbor:** Is an example of a lazy learner; not because of its apparent simplicity, because it memorizes the training dataset instead of learning a discriminative function from the training data. The algorithm beings by choosing a distance metric, then the KNN algorithm finds the specified k samples in the training dataset that are closest to the point that we want to classify. The class label of the new data point is then determined by a majority vote among its k nearest neighbors.

**Naïve Bayes:** Is a classification algorithm, which is based on Bayes’ rule of conditional probability. It makes the assumption that each feature is conditionally independent from each other. As it is often stated, Naive Bayes is not so naive as the algorithm assumes independence between the features, which may be to simplistic and not always hold true, but it works surprisingly well in practice. Naïve Bayes algorithm works by taking all of the evidence available (attributes) in order to modify the prior probability of the prediction. The prior probability is defined as the probability of the hypothesis without knowing any of the evidence. Therefore, using Bayes Rule and assuming that the features are independent (given the class), their combined probability is obtained by multiplying the prior probability and the probabilities of the new evidence (testing example) given the hypothesis.

<img src = "https://tsmith5151.github.io/Blog/img/ML_Concepts/NB.png">

***Pros/Cons:*** One drawback when using NB is the model may be too simplistic and under-fit the data given smaller datasets. Also, if a particular attribute value does not occur in the training set with every class value, things go badly. One advantage of using NB is that the computational complexity is lower than other methods such as decision trees and therefore is quite fast to run and does not require a lot of CPU memory. 


**Support Vector Machine:** Is a classification algorithm that constructs hyperplanes in a high dimensional space and separates two different class labels. There are many possible solutions on where to draw the decision boundary, such as the solid line on the left or to the right. SVM's reduces the risk of selecting the wrong decision boundary by choosing the line that has the largest distance from the bordering data points of the two classes. Our optimization objective is to maximize the margin. The margin is defined as the distance between the separating hyperplane and the training samples that are closest to this hyperplane, which are the so-called support vectors. The dashed line in the middle of the margin width would be where the decision boundary would be. So the idea to have decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting. Thus having the additional space between the groups reduces the chance of selecting the wrong class. 

Finally, note that we would not be able to separate samples from the positive and negative class very well using a linear hyperplane as the decision boundary via linear SVM models. Therefore, implementing the kernel trick, we can solve non-linear classification problems. The basic idea behind kernel methods to deal with such linearly inseparable data is to create nonlinear combinations of the original features to project them onto a higher dimensional space via a mapping function where it becomes linearly separable.


<img src = "https://tsmith5151.github.io/Blog/img/ML_Concepts/SVM.png">
Source: Python Machine Learning (Raschka)

