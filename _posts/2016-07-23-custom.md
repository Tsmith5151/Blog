---
layout: post
section-type: post
title: Unsupervised Learning Methods to Cluster Customers by Spending Patterns
category: technology
tags: [ 'machine learning', 'K-Means','GMM','Principal Component Analysis' ]
author: Trace Smith
---

This blog will disucss in detail the steps taken to analyze a dataset containing annual spending amounts of customers for a whole-food distributing company in order to understand the variation in the different types of customers that the company interacts with. Given the dataset, it shows monetary consumption for different types of food and supplies; statistical analysis and clustering techniques utilized in this project will be a proxy for approximating the volume of supplies different types of customers buy in
addition to uncovering spending patterns of the distributing company's client base. The complete source code and outputs can be found [here](https://github.com/Tsmith5151/Customer-Segments/blob/master/customer_segments.ipynb).


### Python Libraries:

First, a list of the python packages that was utilized in the work is shown below. Note, I will be calling the Machine Learning classifiers and data mining tools from [Scikit-Learn](http://scikit-learn.org).


```python
# Import libraries: NumPy, pandas, matplotlib
import numpy as np
import pandas as pd
import scipy 
import matplotlib.pyplot as pl
import seaborn as sns; sns.set(color_codes=True)
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA, FastICA
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from IPython.display import HTML
```

```python
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
```

### Load Dataset:

```python
data = pd.read_csv("wholesale-customers.csv")
print "Number of Attributes: %s" % (data.shape[1])
print "Number of Observations: %s" % (data.shape[0])
X=data.ix[:,0:6].values
```

```python
data.head() #consumption in m.u
```
<img src = "https://tsmith5151.github.io/Blog/img/Customer/data.png">


### Explore the Data:

Below is some exploratory data analysis which helps us have an understanding
of the data we are working with:

```python
# Descriptive Statistics
data.describe()
```

```python
#Histogram
data.plot(kind='hist',alpha=0.8,bins = 30, subplots=True, layout=(3,2), legend=True, figsize=(12,10))
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/hist.png">

```python
#Bivariate Plot
g = sns.pairplot(data, diag_kind="kde",markers="o", 
        palette=dict(Fresh="g", Milk="m", Frozen="b", Detergents_Paper="y",Delicatessen="r"),
        plot_kws=dict(s=10, edgecolor="g", 
        linewidth=1),diag_kws=dict(shade=True))
g.set(ylim=(0,50000))
g.set(xlim=(0,20000))
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/scatter.png">


### Principal Component Analysis:

Lets discuss both of these methods in detail and discuss how we can implement both
PCA and ICA with our dataset. First, Principal Component Analysis (PCA) is utilized 
to identify patterns in dataset based on the correlation between the n-features. PCA 
attempts to find the directions of maximum variance in a high-dimensional data (d) and then
projects it onto a new subspace (k<=d). The two key insights for PCA is function
`data.var()` which illustrates the features that capture the most variance
(spread of the data distribution); note that fresh has the highest variance.
Also, `data.corr()` since PCA deals with correlation between variables that
create the individual eigenvectors. Both of these functions are found in the
following PCA section. We would expect the first several PCs to be composed of
original attributes that are in some way correlated with each other. These
principal components could represent the customers that purchases a combination
of items from the wholesale grocery distributor. 


Additionally, before running any PCA, plotting the histograms (shown below)
could suggest a correlation in the number of orders among `Fresh`, `Milk`, and
`Grocery`. Intuitively, the histograms show an exponential decline in the number
of orders for the respected products, hence this could perhaps be a cluster
consisting of larger size companies with higher purchase quanitites for these
particular items. In contrast to the remaining products, the histogram virtually
drops off after the first two bins (lower number of orders) and perhaps
indicates the smaller sized companies. In addition to `Fresh` and `Grocery`
having the highest standard deviations of 12,647 and 9,503 respectively, I would
expect these two items to have a high significance (magnitude) in the
eigenvector. What we can conclude from the histograms is that the variables have
logarithmic distributions, so taking the log of all of the variables would
probably give a more normal looking distribution for each variable. A better way
of analyzing correlations between two features would be to examine bivariate
plots (shown in the PCA section) and observe any linear relationships. For
instance, if there is a positive correlation between milk and grocery items,
this means that as purchase orders of milk increases, so does grocery items.
Similarly, as grocery items increase, so does detergents_paper (resembling a
grocery store).

```python
# Correlation Matrix:
data.corr()
```

```python
def corrplot():
    fig, ax = pl.subplots(figsize=(10,8))
    pl.title("Correlation Plot",fontsize=16)
    sns.corrplot(data)
corrplot()
```
<img src = "https://tsmith5151.github.io/Blog/img/Customer/Corrplot.png">


PCA is a widely used technique designed to reveal the underlying structure presumed to exist within a set of multivariate measurements. PCA is a method for compressing a lot of data into
something that captures the essence of the original data. Given a large dataset
with multiple attributes, the question is are all of the dimensions important
(i.e. features) or are some more important that others? PCA is strictly a
mathematical procedure that results in the same number of principle components
as there were input variables, but uncorrelated. There are two major features of
PCA; through algebraic projection, it represents the original data in a new data
space with the same order. The axes (principal components) in the new data space
are orthogonal to each other. The newly formed axes are ordered in a
sequentially reduced fashion in terms of their weight; which simply means that
any given principal component captures more variability than the one that
immediately follows it and so forth. Principal components are nothing more than
the extracted eigenvectors and eigenvalues from a correlation matrix. Using a
non-mathematical definition, one can view the eigenvectors as the slope of axes
defining a series of ellipses, with the axes always orthogonal to each other
(see figure below).

![alt](http://webhelp.esri.com/arcgisdesktop/9.1/published_images/Multivariate_p
rincomp1b.gif)

***Covariance***: First, lets begin by constructing the covariance matrix (S).
The covariance is the measures how much the features vary from the mean with
respect to each other. First, variance and covariance are a measure of the
"spread" of a set of points around the mean. Variance measures the deviation
from the mean of points in one dimension while covariance measures how much each
of the dimensions vary from the mean with respect to each other. Note that the
covariance between one dimension and itself is the variance. Covariance is
measured between two dimensions (attributes) to see if there is a relationship
between the two dimensions like the number of hours studied and the number of
questions a student got wrong on the exam. So for example, suppose we have a 3x3
matrix (x,y,z) then we can measure the covariance between the x and y
dimensions, y and z dimensions, and x and z dimensions. Measuring the covariance
between x and x, y and y, z and z result in the variance of x,y, and z
respectively. Hence, this would be the main diagonal of the covariance matrix.
Also, Cov(x,y) = Cov(y,x), hence the matrix is symmetrical about the main
diagonal (as shown below). Also, the sign of the covarince value is very
important. A positive value indicates both dimensions increase or decrease
together. A negative value indicates one dimension increases while the other
decreases. Also, if the covariance is zero, then the two dimensions are
independent of each other. This example only consists of three dimensions.
However, covariance calculations are very useful when finding relationships
between features in higher dimensional spaces (i.e. greater than three) where
visualization is difficult.

![alt](http://i.stack.imgur.com/ByTyM.png)

The covariance matrix is a dxd matrix where each element represents the
covariance between two attributed.

Covariance is calculated as follows: 

<img src = "https://tsmith5151.github.io/Blog/img/Customer/covariance.png">


The covariance matrix can be computed via the following matrix equation:

<img src = "https://tsmith5151.github.io/Blog/img/Customer/matrix.png">


***Note:*** the mean vector is a d-dimensional vector where each value in this
vector represents the sample mean of a the given feature column:

<img src = "https://tsmith5151.github.io/Blog/img/Customer/mean_vector.png">


The below plot illustrates how the covariance matrix defines the shape of the
data. The diagonal spread is captured by the covariance, while axis-aligned
spread is captured by the variance. If we create a scatter plot of Grocery vs
Detergents_Paper and compute the coviariance matrix, we see that the off-
diagonal value is positive, therefore as x increases, so does y.


```python
# Covariance Matrix:
cov_matrix = data.cov()
cov_matrix
```

```python
# Import Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn import linear_model

def scatter_plot_corr(x,y):
    data = np.array([x,y])
    print "Variance: %s\n" % (np.var(data))
    print "***Covariance Matrix:***\n"
    print np.cov(data)
    # Find best fit line (SGD)
    x = np.array([x]).T
    y = np.array([y]).T
    reg = LinearRegression()
    fit = reg.fit(x,y)
    pred = reg.predict(x)
    #Generate Plot
    pl.figure(figsize=(8,6))    
    pl.scatter(x,y,s=30,color='g')
    pl.plot(x,pred,color='blue',linewidth=2.5)
    pl.xlabel('X',fontsize=12)
    pl.ylabel('Y',fontsize=12)
    pl.title('Example: X vs Y', fontsize=16)

    pl.show()
    
def run():
    # y=mx+b
    x = data['Grocery']
    y = data['Detergents_Paper']
    scatter_plot_corr(x,y)
    
if __name__ == '__main__':
    run()
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/PCA_ill2.png">


***Eigendecomposition of the covariance matrix***: The covariance matrix is
decomposed into its eigenvectors and eigenvalues. Note that the eigen-
decomposition of the covariance matrix (assuming input data was standardized)
yields the same results as a eigende-composition on the correlation matrix,
since the correlation matrix can be understood as the normalized covariance
matrix. In this work, we will be utilizing the covariance matrix. The
eigenvectors of S represents the principal components (directions of maximum
variance and determined the direction of the new feature space) and the
eigenvalues (scalar) correspond to the magnitude of the eigenvectors; in other
words, the eigenvalues explains the variance along the new feature axes.
Eigenvectors can be used to create a linear equation, much like a regression
equation, to create new properties and to help classify the original data
samples into different groupings. The eigenvector with the largest eigenvalue is
the direction along which the data set has the maximum variance. For example, if
the first principal component (eigenvector 1) has an eigenvalue = 5.45 and there
are 10 variables, then this eigenvalue and the first component accounts for
54.5% of the total variance (5.45/10). The eigenvectors are used to create new
properties. The eigenvalues are sorted in descending order (largest first) and
each eigenvalue is divided by the total variance, yielding the fraction of
variance explained. There are as many eigenvectors as there are variables(i.e.
attributes). The total variance is then the Trace of the S matrix, which is just
the summation of the main diagonal. Eigenvalues are then determined from the
axes lengths and there are as many eigenvalues as there are variables; each
eigenvalue accounts for a certain percentage of the total variance. For
instance, if the first three principal components together account for most of
the variance, then the number of features in the multidimensional space is
reduced from 10 to 3. Principal component 2 (eigenvector 2) is orthogonal
(uncorrelated) to the first component and accounts for the second highest
variance. In this case principal component 10 accounts for the least amount of
the total variance.


So again, the objective here is to reduce the dimensionality of the data by
compressing it onto a new feature subspace, we chose only the subset of the
eigenvectors (principal components) that contains most of the information
(variance). The eigenvector corresponding to the highest eigenvalue will be the
first PC and will point in the direction of maximum variance (most information
gained) of the dataset. The second eigenvector (v2) will point in direction
orthogonal (perpendicular in 2D) to v1. One of the main advantages of using PCA
is if you have a dataset with high dimensions you can reduce the number
dimensions without losing much of the information. Using the projection matrix
(i.e. a matrix of our concatenated top eigenvectors), the data is projected onto
a new feature space and thus reducing the number dimensions through data
compression. By choosing the top eigenvectors (i.e. PC's that contains the
highest percentage of explained variance) with the highest eigenvalues to
construct our dxk (where k is the number of PCs) dimensional eigenvector matrix
W. In the projection onto the new feature space step, we use the dxk-dimensional
projection matrix W to transform our samples onto the new subspace via the
equation Y = XxW, where X= number of elements (i.e. N).

![alt](https://onlinecourses.science.psu.edu/stat857/sites/onlinecourses.science
.psu.edu.stat857/files/lesson05/PCA_plot.gif)


The first PC accounts for the highest portion of the total variance or
"spread" of the data. The subsequent PC's account for the remaining variability,
and usually the first couple of PC's account for roughly 90-95% of the total
variance. The below plot shows the variance vs the number of principal
components (k=6). Based on this plot, the number of dimensions would be reduced
to 2, which explains roughly 86% of the total variance. We could use also use
the third PC to explain more of the variance, but for visualizing the data in
2D, only the first two PCs will be considered. In this plot, the ['Explained
Variance Ratio`](http://scikit-
learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
is a fraction of the total variance explained by each of the selected
components. In regards to how quickly the variance drops off, we can compute the
slope of the number of PC components (x) vs the explained variance (y). The
slope between PC 1-2 is -0.054 and the slope between 2-3 is -0.335. Furthermore,
the slope between PC 3-4 is -0.026 and at this point, the slope begins to level
out. Therefore between PC 2-3 you observe the steepest slope as the explained
variance ratio begins to quickly decline. The bar plot below is an alternative
visualization to observe the change in variance versus the number of principal
components.


As a note, for PCA to be implemented correctly, the dataset needs to be
standardized (mean equal 0 and standard deviation =1). This needs to be
addressed in the event where features are on different scales or different
magnitudes of range, and thus causing the results of PCA to be biased in the
direction of the features with a larger range. One approach to scaling the data
is using [`Scale`](http://scikit-
learn.org/stable/modules/generated/sklearn.preprocessing.scale.html); this
function centers the mean and component wise scale to unit variance. However, it
may not always be the case in which the data should be standardized. In some
instances you may want to preserve the variance of each dimension as opposed to
scaling to unit length, which could remove the potential dependence between
features since each feature is scaled independently. The features were not
scaled to stdev = 1 for this dataset as the attributes have the same units
monetary units (m.u.) and in essence, we are trying to capture the variance in
spending among the different types of customers.

**PCA: Python Code**

Now in the regards to this work, the original dimensional space the dimensions
of the axes are in terms of the amount of monetary units (m.u.) the food
distributor sales. The axes are then transposed into the new dimensional space
and is now represented by the principal components.  For this dataset, there are
440 elements (n=440) and 6 measurements (m=6), so the covariance matrix would be
a 440x6 matrix with 6 real eigenvalues; the remaining 434 eigenvalues are zero.
Now, after applying the linear PCA transformation, we have a lower dimensional
subspace where the samples are “most spread” along the new feature axes. The PCA
code is shown in the following code blocks:


```python
## In Progress....

def standardize(X):
    X_std = StandardScaler().fit_transform(X)
    return X_std
    
def covariance(X):
    #Covariance Matrix:
    X_mean = np.mean(X,axis=0)
    n = X_std.shape[0]
    cov = ((X - X_mean).T.dot(X - X_mean))/(n-1)
    print "Covariance Matrix: \n%s" % (cov)

def eig_vec_vals(X_std):
    #eigen-decomposition on the covariance matrix:
    eig_vals, eig_vector = np.linalg.eig(cov)
    print '\nEigenvectors: \n %s' % (eig_vector)
    print '\nEigenvalues: \n %s' % (eig_vals)
    
def svd(X_std):
    u,s,v = (np.linalg.svd(X.T))
    p = PrettyTable()
    for row in np.round(u,3):
        p.add_row(row)

    print p.get_string(header=False,border=True)

def variance(eig_vals):
    #Explained Variance Ratio
    total = sum(eig_vals)
    explained_var_r = [(i/total)*100 for i in sorted(eig_vals,reverse=True)]
    x = range(1,7)
    data_ = pd.DataFrame(zip(x,explained_var_r),columns=['PC','Variance Ratio'])
    print "\nExplained Variance Ratio: \n %s" % (data_)

    
def run():
    X = data.ix[:,0:6].values
    X_std = standardize(X)
    #standardize(X)
    #covariance(X)
    #eig_vec_vals(X) 
    #variance(eig_vals)
    #svd(X_std)
    
if __name__ == '__main__':
    run()
```

**PCA - Using Sklearn**

```python
def standardize_data():
    #Subtracting out the mean to center the data:
    columns = data.columns
    mean = data.mean()
    df_m = pd.DataFrame(mean)
    df_mean = df_m.transpose()
    data_center = pd.DataFrame(data[columns].values - df_mean[columns].values, columns=columns)
    #print data_center

def doPCA():
    pca = PCA(n_components=5)
    pca.fit(data_center)
    return pca

def PCA_SKLEARN():
    # Print the components and the amount of variance in the data contained in each dimension
    pca = doPCA()
    columns = data.columns
    df_ica = pd.DataFrame(pca.components_, columns = columns, index=['1', '2', '3', '4','5'])
    df_ica.index.names = ['PC']
    print "Principal Component Analysis:"
    print df_ica

    ex_var = pca.explained_variance_ratio_
    df_var = pd.Series(ex_var,index=['1','2','3','4','5'])
    df_var.sort(ascending=False)

    df_var.index.names = ['PC']
    print "\nExplained Variance of Each Component:"
    print df_var

def scree_plot():
    #Scree Plot:
    x = np.arange(1,6)
    y1 = ex_var
    y2 = np.cumsum(ex_var)

    pl.figure(figsize=(16,18))
    fig, ax1 = pl.subplots(1)
    ax2 = ax1.twinx()

    #Generate Curves
    ax1.plot(x,y1,color = 'green',linewidth=2.5, linestyle='--',marker='o')
    ax2.plot(x,y2,color = 'blue',linewidth=2.5,marker='o')

    #Set Axes Labels:
    ax1.set_xlabel("No. of PC Components", fontsize = 14)
    ax2.set_ylabel("Cumulative Explained Variance Ratio",fontsize =14)
    ax1.set_ylabel("% Explained Variance", fontsize =14)

    #Set Limits:
    ax2.set_ylim(0.4,1,.2)
    ax1.set_ylim(0,.6,.2)
    ax1.set_xlim(1,6,1)

    pl.title("Scree Plot", fontsize = 16)
    pl.legend()
    pl.show()
    
def variance_hist():
    pl.figure(figsize=(8,6))
    pl.bar(range(1,6), ex_var, alpha = .8, align='center',
           label = 'Individual Explained Variance', color = 'g')
    pl.ylabel('Explained Variance Ratio', fontsize = 14)
    pl.xlabel('Princial Components', fontsize = 14)
    pl.title('Explained Variance', fontsize = 16)
    pl.show

    #Slope:
    X1, Y1 = 1, 0.459614
    X2, Y2 = 2, 0.405172
    X3, Y3 = 3, 0.070030
    X4, Y4 = 4, 0.044023
    slope1 = (Y2-Y1)/(X2-X1)
    slope2 = (Y3-Y2)/(X3-X2)
    slope3= (Y4-Y3)/(X4-X3)
    #print "Slope1:", slope1
    #print "Slope2:", slope2
    #print "Slope3:", slope3

def run():
    standardize_data()
    PCA_SKLEARN()
    scree_plot()
    variance_hist()

if __name__ == '__main__':
    run()
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/PCA.png">

Below is a biplot and this plot illustrates is how each feature is composed
within the first two principal components (i.e. the first and second PC's).
There are a total of 440 points, with each point being a customer of the
distributing company. We can see from this plot that 'Fresh' mostly dominates
the first principal component, since it extends in large magnitude towards -1,
however, since it is nearly horizontal, it is not a contributing factor in the
second principal component. 'Frozen' plays a similar role as a feature to
'Fresh' but with less magnitude (which may indicate a redundant feature, in some
sense).

```python
def biplot(df):
    # Fit on 2 components
    #Scaling 
    pca = PCA(n_components=2, whiten=True).fit(df)
    
    # Plot transformed/projected data
    ax = pd.DataFrame(
        pca.transform(df),
        columns=['PC1', 'PC2']
    ).plot(kind='scatter', x='PC1', y='PC2', figsize=(8, 6), color = 'g', s=8)
    # Plot arrows and labels
    for i, (pc1, pc2) in enumerate(zip(pca.components_[0], pca.components_[1])):
        ax.arrow(0, 0, pc1, pc2,linewidth=3.0, width=0.001, fc='r', ec='b')
        ax.annotate(df.columns[i], (pc1, pc2), size=12)
    return ax

ax = biplot(data)
pl.title("Biplot", fontsize = 20)
pl.xlabel("Principal Component: 1", fontsize = 14)
pl.ylabel("Principal Component: 2", fontsize = 14)
ax.set_xlim([-1.5, .5])
ax.set_ylim([-1.0, 1.5])
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/biplot.png">


Based on the PCA results shown above, the dimensions of the first PC (explains
the most variability in the data) and has coefficients of `Fresh = -0.976`,
`Frozen = -0.152`,`Milk = -0.121`, etc. We can clearly see that fresh items have
the largest magnitude, followed by frozen products, and then milk. The
descriptive statistics shows that on average, fresh items are purchased 55% more
than the other 5 products. The annual spending on fresh items has the largest
standard deviation (12,647) which suggest there is a lot of variance between the
customers on how much they spend on fresh items. As the first PC identifies the
item as the most important factor among the features, it would be a good idea to
increase marketing towards the customer base for fresh products as it seems to
be a cluster of those who buys fresh produce in bulk quanitites.


Second Principal Component gives the following magnitudes for the eigenvector
v2: `Fresh = -0.110614` `Milk =0.515802`, `Grocery =0.764606`, `Frozen =
-0.018723` etc. Looking at the magnitude of the eigenvector, a correlation exist
among `Grocery` and `Milk`; customers who buy more 'grocery' items also tend to
buy more 'milk' items as well. Therefore, it would be advised to market grocery
and milk products together given their correlation in order to increase sales.
Bundling these two items when shipping to the customer will also reduce cost.


In summary, the 1st principal component is strongly correlated with the
"Fresh" variable. The 1st principal component increases as the "Fresh" variable
decreases. This component can be viewed as a measure of how "Fresh" orders
decline with high-volume customers. Because of the high correlation for "Fresh"
(-0.97), this principal component is primarily a measure of the "Fresh"
variable. Therefore, high-volume customers order much less "Fresh" items (in
comparison to other items) than low-volume customers.

### Independent Component Analysis:

***Independent Component Analysis*** is a statistical technique for
identifying the underlying hidden factors for a given multidimensional dataset.
In short, ICA produces dimensions of variation where the dimensions (features)
are statistically independent from one another. The data variables are assumed
to be linear mixtures of some unknown "latent" variables and the mixing system
is also unknown. The assumption is that the latent variables are both
nongaussian and mutually independent; these variables are called the independent
components. In the context of this project we are attempting to identify the
different buyer types and cluster them together. Being statistically
independent, the different types of buyers could mean the size of the purchaser
(small/larger) or different consumption patterns. For example, there are buyers
who mostly purchased Milk and Grocery, and less Detergents/Paper or
Delicatessen. Likely this could represent the type of purchasers for a small
market or convenience store.


[`Independent Component Analysis`](http://scikit-
learn.org/stable/modules/decomposition.html#ica) attempts to find
directions in the feature space corresponding to projections with high non-
Gaussianity. ICA will provide six new vectors with each vector being an
independent component. Furthermore with ICA, we can attempt to identify the
different types of customers so that we can understand more of their consumption
patterns. In ICA, we are taking into account the positive and negative signs of
the coefficient as they are considered anti-correlated to each other. This
simply means that if we have a high positive coefficient and a very negative
coefficient, it would be interpreted as one or another but not both, hence anti-
associated to each other.


Before running ICA, we can use a preprocessing technique such as
[`Scale`](http://scikit-
learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) to center
the data to zero and gives each column a variance of 1; removing the mean can
help measure the variance of the data now from the origin (origin). This could
still be done by keeping the mean, the only difference now is that the center is
the mean itself, and not 0. The calculations are more simpler when you remove
the mean as oppose to keeping it. Also, having the values centered at zero can
capture the distributions when transforming the data.


**ICA1:** Mostly includes frozen items (highest relative magnitude); the
customers tend to buy more grocery and frozen items and less
detergents_paper, delicatessen, fresh, and milk products. Possibly could resemble
smaller sized grocery store where larger quanities of grocery/frozen foods are
majority of the purchase orders for this particular cluster of clients.

**ICA2:** As shown the the below heatmap, we see that grocery items has the
strongest component. We see that customers who buy more fresh produce also buys
less milk, detergents_paper, and delicatessen. This could suggest a smaller
restaurants, where majority of the demand is for fresh/grocery/frozen


**ICA3:** The most significant purchase orders from the distributor is for
delicatessens while decrease in the remaining products. This suggests a group of
customers that own a deli/sandwich shop.


**ICA4:** In this IC, grocery has the highest postive relative magnitude
(0.11) while in contrast, detergents_papers has the lowest negative magnitude
(-0.13). This is an example where a component has a large positive coefficient
and a relatively large negative coefficient (compared to the remaining
features). There is an anti-correlation between grocery and detergent purchases.
Possibly this could come from a distinction between something like grocery
stores and pharmacies which carry some food but far more paper products. It
would be interesting to see what time of year where you see large increases in
grocery items and less in detergents_paper and vice versa. Could this possibily
correspond to the holiday seasons where demand for grocery items tends to
increase.


**ICA5:** Customer purchases mostly milk/grocery will buy less slightly less
fresh items and detergents (possibly a convenience store)


**ICA6:** Primarly as purchase orders for grocery increases, milk/fresh
decrease. This group of looks very similar to ICA5 and follows trends that more
closely look like a convenience or small market store.

```python
#Adjust the data to have center at the origin:
columns = data.columns
scale = preprocessing.scale(data)
df_scale = pd.DataFrame(scale, columns=columns)
#print data_scaled.mean()
#print data_scaled.std()

def doICA():
    ica = FastICA(n_components = 6, random_state=42)
    ica.fit(df_scale)
    return ica

# Print the independent components
ica = doICA()
```

```python
df_ica = pd.DataFrame(ica.components_,columns=columns, index=['1','2','3','4','5','6'])
df_ica.index.names = ['IC']
print "\nUnmixing Matrix:"
df_ica
```

```python
pl.figure(figsize=(20,20))
pl.figure(figsize = (11,5))
pl.title('Independent Component Analysis', fontsize=18)
sns.heatmap(df_ica, annot = True)
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/ICA1.png">

```python
# Another way we can veiw ICA
df_ica.plot(kind = 'bar', figsize = (10, 6))
pl.title('Independent Component Analysis', fontsize=18)
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/ICA2.png">

<br>

### Clustering:

In unsupervised learning, we are not predicting an outcome (no target label is
given "y"); we are interested in how the features are related. We can group the
data into "clusters" and two common techniques are K-means and Gaussian Mixture
Models. First, K-mean is an iterative algorithm that is based on the idea that there exist a
specific number of clusters or groups in the dataset and finds clusters that
consists of similar characteristics that are more related to each other than in
other groups. Each group in the data is distributed around a central point
called the "centroid" which is the average of the cluster. K-means begins by
initially guessing the K-number of clusters and then the algorithm randomly
picks the centroid as the initial clusters of the data (not the correct centers
for the first iteration). All of the points are assigned to the nearest centroid
based on the Euclidian distance and is grouped into a cluster. The new centroid
will be the mean of data points assigned to the corresponding cluster. This
process is repeated until either the number of specified iterations is reached
or until the clustering assignments do not change. Using K-Means in scikit-
learn, the iteration will stop early if it converges before the maximum number
of iterations is reached. As a note, incorrectly assigning the number of
clusters for k can result in poor clustering performance. Next, each sample is
assigned to the nearest centroid and then move the centroid to the center of the
corresponding samples.


K-Means is a hard clustering algorithm, where the clusters to not overlap
(i.e. the data point is either "blue" or "green". The advantages of using
K-means is due to it's simplicity, speed, and scalability of larger numbers of
data points. However, one weakness when using this method is that the initial
centroids are placed randomly, which could result in a bad starting spot and the
iteration could stop at an unlikely solution. Thus numerous iterations need to
be run which can come at a cost of more time required to run K-means, however
the trade-off is that more iteration that are conducted, the better the results
will be.


A generalization of K-Means algorithm is Gaussian Mixture Models (GMM). GMM
assumes that all of the data points in each cluster are generated from a mixture
of a finite number of Gaussian distributions with unknown parameters. GMM use a
Gaussian distributions to find the most probable cluster that a point would
belong to. The mixture model attempts to find unknown parameters such as the
mean/covariance of the Gaussian distribution. The parameters of the GMM are
estimated by the maximum likelihood criterion using the Expectation-Maximization
(EM) algorithm. EM algorithm is very sensitive to the initialization of the
model, thus it can require a longer time to converge if there is a poor initial
guess. Another disadvantage of the GMM algorithm is that can fail if the
dimensionality of the dataset is too high (i.e. greater d=6). In many cases, how
many mixture models should be used is unknown and a different number of mixture
models (n_components) will to have to be experimented with. Alternatively, one
advantage of using mixture models (Gaussian distribution) it is a
probabilistically way of doing soft clustering. Soft clustering is often desired
as it provides uncertainties on the assignment of data points to a given cluster
rather than K-means, which classifies the point of interest to belong to only a
single cluster. Another strength of using mixture models are that the model
covers the data well and that a density estimation for each cluster can be
obtained. Therefore, GMM will be considered in the analysis, however in the code
below, results for both K-Means and GMM are shown. The two main differences between the two models are speed and structure.

**Speed:**
K-Mean much faster and much more scalable
GMM slower since it has to incorporate information about the distributions
of the data, thus it has to deal with the co-variance, mean, variance, and prior
probabilities of the data, and also has to assign probabilities to belonging to
each clusters.


**Structure:**

K-Means straight boundaries (hard clustering)
GMM you get much more structural information, thus you can measure how
wide each cluster is, since it works on probabilities (soft clustering)


***Below is the code to visualize the clusters (Gaussian Mixture Model &
K-Means)***

```python
# Import clustering modules
from sklearn.cluster import KMeans
from sklearn.mixture import GMM
```

```python
# TODO: Reduce to Two dimensions using PCA to capture variation
reduced_data = PCA(n_components = 2).fit_transform(data)
df_rd = pd.DataFrame(reduced_data)
df_rd[:11]
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/PCA_2.png">

In order to evaluate the quality of the clustering, we need to find the
optimal number of clusters. This can be done by computing the inertia, a method
for comparing the performance of different k-means clusters (n_clusters).
Inertia is calculated by taking the summation of the difference between every
data point in each cluster and it's respected centroid. Inertia is accessible is
calling the `inertia__` function, which computes the within cluster SSE. If the
data points in the cluster are similar to it's centroid, the difference is
small, resulting in a small inertia. Plotting the number of clusters vs the
inertia yields the plot below. What is important from this plot is to observe
the rate of change. We see that as the number of clusters increase, the lower
the inertia will be. This is due to the samples being closer to their
corresponding centroids. Basically, the idea behind this plot is to identify the
value of k where the inertia begins to increase most rapidly, thus the optimal
number of clusters to consider in this model is between 4-5.


As GMM was ultimately chosen, [`BIC score`](http://scikit-
learn.org/stable/auto_examples/mixture/plot_gmm_selection.html) is more
appropriate to determine the number of clusters. Bayesian information criterion
or (BIC). Most often the case, the number of components for the mixture model
and covariance are unknown. One way to tune a GMM is by comparing information
criteria and the two most popular information criteria are Akaike's Information
Criterion (AIC) and (BIC). In this analysis, we will consider only the BIC
score. The model giving smallest BIC score is selected as the best model,
however BIC tends to choose simpler models that may underfit the data. We can
conclude from the BIC histogram, the models perform the best when for
n_components = 5. More on Clustering Using Gaussian Mixture Models can be found
[`here`](http://www.mathworks.com/help/stats/clustering-using-gaussian-mixture-
models.html).


```python
def number_clusters():
    inertia = []
    delta_inertia = []
    for i in range(1,11):
        clustering = KMeans(n_clusters = i,
                    n_init=10,
                    max_iter=300,
                    random_state=1)
        clustering.fit(reduced_data)
        if inertia: 
            delta_inertia.append(inertia[-1] - clustering.inertia_)
        inertia.append(clustering.inertia_)
    
    pl.figure(figsize=(8,6))
    pl.plot([k for k in range(1,10)], delta_inertia, marker='o', color='b',linewidth=3.5)
    pl.title('Optimal Number of Clusters', fontsize=18)
    pl.xlabel('Number of Clusters', fontsize=14)
    pl.ylabel('Rate of Change (Inertia)', fontsize=14)
    pl.show
    
number_clusters()
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/NumCluster.png">

```python
#This plot was generated using the sample code from: 
#http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html#example-mixture-plot-gmm-selection-py

import itertools

lowest_bic = np.infty
bic = []
n_components_range = range(1, 11)
cv_types = ['spherical', 'tied', 'diag', 'full']
for cv_type in cv_types:
    for n_components in n_components_range:
        # Fit a mixture of Gaussians with EM
        gmm = GMM(n_components=n_components, covariance_type=cv_type)
        gmm.fit(reduced_data)
        bic.append(gmm.bic(reduced_data))
        if bic[-1] < lowest_bic:
            lowest_bic = bic[-1]
            best_gmm = gmm

bic = np.array(bic)
color_iter = itertools.cycle(['c', 'r', 'g', 'b'])
bars = []            

ax = pl.figure(figsize=(10,6))
for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):
    xpos = np.array(n_components_range) + .2 * (i - 2)
    bars.append(pl.bar(xpos, bic[i * len(n_components_range):
                                  (i + 1) * len(n_components_range)],
                        width=.2, color=color, alpha=0.6))
pl.xticks(n_components_range)
pl.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])
pl.title('BIC Score Per Model', fontsize=18)

xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\
    .2 * np.floor(bic.argmin() / len(n_components_range))
pl.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)
pl.xlabel('Number of components', fontsize=14)
pl.ylabel('BIC Score', fontsize=14)
pl.legend([b[0] for b in bars], cv_types)
```
<img src = "https://tsmith5151.github.io/Blog/img/Customer/BIC.png">

<br>

### Gaussian Mixture Model

```python
def cluster_plot_gmm():   
    n = 5
    # TODO: Implement clustering algorithm and fitting to the reduced data for visualization
    gmm = GMM(n_components=n, n_iter=300)
    clusters = gmm.fit(reduced_data)
    centroids = clusters.means_
    
    x_gmm = clusters.predict(reduced_data)

    # Which customers are in which cluster: K-Means
    for i in range(0,5):
        count = data[x_gmm==i].shape[0]
        print 'Gaussian Mixture Model:'
        print 'Cluster Number: %s' % (i+1)
        print 'Number of samples in cluster: %s' % count
        print data[x_gmm==i][list(data.columns[:-1])].mean()

    # Plot the decision boundary by building a mesh grid to populate a graph.
    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
    hx = (x_max-x_min)/1000.
    hy = (y_max-y_min)/1000.
    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))

    # Obtain labels for each point in mesh. Use last trained model.
    Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    pl.figure(figsize=(8,6))
    Z = Z.reshape(xx.shape)
    pl.figure(1)
    pl.clf()
    pl.imshow(Z, interpolation='nearest',
                extent=(xx.min(), xx.max(), yy.min(), yy.max()),
                cmap=pl.cm.Paired,
                aspect='auto', origin='lower')

    pl.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=5)
    pl.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=150, linewidths=4,
                color='w', zorder=10)
    pl.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\n'
            'White X=Centroids; No. of Clusters:%s'%(5), fontsize = 14)
    pl.xlim(x_min, x_max)
    pl.ylim(y_min, y_max)
    pl.xticks(())
    pl.yticks(())

    # Annotate Plot:
    labels = ['{0}'.format(i) for i in range(1,6)]
    for label, x, y in zip(labels, centroids[:,0], centroids[:,1]):
         pl.annotate(label, xy = (x, y), xytext = (-20, 20),
         textcoords = 'offset points', size=14, ha = 'right', va = 'bottom',
         bbox = dict(boxstyle="round4,pad=0.3", fc="cyan", ec="b", lw=2),
         arrowprops = dict(arrowstyle = 'simple', connectionstyle = 'arc3,rad=-0.3'))
    pl.show()
    
    data['cluster'] = clusters.predict(reduced_data)
    results = pd.DataFrame(data=labels, columns=['cluster']) 
    data_grouped = data.groupby('cluster').mean()
    
    # Histogram
    ax = data_grouped.plot(kind='bar',legend=True, 
                    figsize=(10,6), alpha=.5, title="Customer Breakdown")
    ax.set_xlabel("Clusters",fontsize=16)
    ax.set_ylabel("Total Monetary Units",fontsize=16)
    ax.set_title(ax.get_title(), fontsize=20)
    pl.show()

cluster_plot_gmm()
```

<img src = "https://tsmith5151.github.io/Blog/img/Customer/GMM1.png">

<img src = "https://tsmith5151.github.io/Blog/img/Customer/GMM2.png">

<br>

### K-Means

```python
def cluster_plot_kmeans():  
    num_clusters = 5
    clusters = KMeans(n_clusters=num_clusters, n_init=100)
    clusters.fit(reduced_data)
    centroids = clusters.cluster_centers_
    
    x_km = clusters.predict(reduced_data)

    # Which customers are in which cluster: K-Means
    for i in range(0,5):
        count = data[x_km==i].shape[0]
        print 'Kmeans:'
        print 'Cluster Number: %s' % (i+1)
        print 'Number of samples in cluster: %s' % count
        print data[x_km==i][list(data.columns[:-1])].mean()

    # Plot the decision boundary by building a mesh grid to populate a graph.
    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
    hx = (x_max-x_min)/1000.
    hy = (y_max-y_min)/1000.
    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))

    # Obtain labels for each point in mesh. Use last trained model.
    Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    pl.figure(figsize=(8,6))
    Z = Z.reshape(xx.shape)
    pl.figure(1)
    pl.clf()
    pl.imshow(Z, interpolation='nearest',
                extent=(xx.min(), xx.max(), yy.min(), yy.max()),
                cmap=pl.cm.Paired,
                aspect='auto', origin='lower')

    pl.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=5)
    pl.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=150, linewidths=4,
                color='w', zorder=10)
    pl.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\n'
            'White X=Centroids; No. of Clusters:%s'%(5), fontsize = 14)
    pl.xlim(x_min, x_max)
    pl.ylim(y_min, y_max)
    pl.xticks(())
    pl.yticks(())

    # Annotate Plot:
    labels = ['{0}'.format(i) for i in range(1,6)]
    for label, x, y in zip(labels, centroids[:,0], centroids[:,1]):
         pl.annotate(label, xy = (x, y), xytext = (-20, 20),
         textcoords = 'offset points', size=14, ha = 'right', va = 'bottom',
         bbox = dict(boxstyle="round4,pad=0.3", fc="cyan", ec="b", lw=2),
         arrowprops = dict(arrowstyle = 'simple', connectionstyle = 'arc3,rad=-0.3'))
    pl.show()
    
    print "K-Means:"
    print "Number of Customers per Cluster:"
    print data['cluster'].value_counts()

    data['cluster'] = clusters.labels_
    results = pd.DataFrame(data=labels, columns=['cluster']) 
    data_grouped = data.groupby('cluster').mean()
    
    # Histogram
    ax = data_grouped.plot(kind='bar',legend=True, 
                    figsize=(10,6), alpha=.5, title="Customer Breakdown")
    ax.set_xlabel("Clusters",fontsize=16)
    ax.set_ylabel("Total Monetary Units",fontsize=16)
    ax.set_title(ax.get_title(), fontsize=20)
    pl.show()

cluster_plot_kmeans()
```
<img src = "https://tsmith5151.github.io/Blog/img/Customer/KM1.png">

<img src = "https://tsmith5151.github.io/Blog/img/Customer/KM2.png">
<br>


### Conclusion:

The central object of each cluster is in reference to the above Gaussian
Mixture Model (GMM) and K-Means cluster plot and histogram. For the sake of this
discussion, we will explore the results from the GMM. As a note on the
histogram, the cluster labeling begins at 0; this refers to cluster 1 and so on.
What we find in the first cluster is a higher precentage of fresh, milk,
grocery, and frozen products but with lowest consumption among the remaining
clusters. To the left of cluster 1 is the second cluster (green) which contains
133 samples. The fresh items are the most significant, which reflects the
results discussed earlier with `fresh` having the highest magnitude in the first
PC, thus more influential. The first two clusters indiciate this customer base
to be a smaller-sized familiy run corner store or convience store with lower
consumption patterns. The third cluster shows larger consumption patterns which
would classify this group as the larger grocery stores such as Whole Foods or
Trader Joes. Only 17 stores fall into this cluster, which is just a small
fraciton of the total customers. The fourth cluster has a total of 38 customers
and the most significant item in this cluster is a high consumption of fresh
products. Consumption for Milk, grocery, and frozen items drops off by nearly
75% in this cluster, perhaps representing the trends associated with a
restaurant/fast-food that has a high density of fresh products. The fifth
cluster consists of a customer base that would most likely resemble a smaller
neighborhood grocery store with the bulk of the products purchased being
fresh/milk/grocery items.

Applying Principal Component Analysis was one of the most useful techniques as
it provides insight into the data. We were able to reduce the high dimensional
dataset to a lower dimensional space which allowed the data to be visualized on
a two dimensional scatter plot. The data was projected onto the new principal
component axes and each datapoint in the plot represents a single class
(customer). Additionally, PCA is a feature extractor, thus we are able to
identify which attributes are most significant in each principal component.
After applying PCA, the second most useful method is then implementing GMM to
the reduced dataset in order to group the customers into respective market
segments, which can enhance better decision making.

As we have classified the wholesale customers into different markets, applying
the A/B test would help the company with making any changes to the business
model. A/B testing, often called split testing, is widely used when creating web
pages by comparing two versions of a web page and see which one performs better.
Likewise, applying A/B testing to this dataset will allow the company to make
optimal decisions by trying out possible changes and seeing what performs
better. For an example, we may want to test various pricing options for products
to monitor how the change in price affects purchase rate for a particular
market. We can split the cluster into two groups with the goal of determining if
the rate of success is higher in one group (A) than the other (B). As a note,
each cluster could have its own A/B test to eliminate any effects of different
customer types or customer sizes. One way to do this is determine the p-value to
get an idea of how confident we are that the two groups have different chances
of success. Using the common baseline of "statistically significant" at 0.05, a
p-value lower than 0.05 means the two groups are different. But the estimated
improvement in success rate would give a better idea of how much better/worse
any changes might be compared to initial prices. There are numerous instances on
what could be analyzed, but this example would just give an idea on how we can
make optimal business decisions by applying any new changes to how the company
operates.

Once we have defined the customer base, we can then target particular
customers by performing different tests in order to enhance profit and growth of
the company. After applying PCA and GMM, we have grouped the data into clusters
and now the company can utilize a supervised machine learning model, either
regression or classification, to predict which cluster the new customer is
likely to fall in. For instance, we could extract the customers and their
corresponding consumption elements (Fresh, frozen, etc.) from the Gaussian
Mixture Model and use then use their respected cluster as the label to use a
classification learning algorithm. This would then allow the company to make
prediction on future customers and their purchase orders and also how they could
make any changes to the distribution operations. Another suggestion would be to
use regression on the clusters that were identified in order to predict purchase
orders from current stores and future wholesale customers.
